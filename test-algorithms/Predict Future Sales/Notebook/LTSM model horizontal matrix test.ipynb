{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\Anaconda3\\envs\\Kaggle\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\Kaggle\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\lucas\\Anaconda3\\envs\\Kaggle\\lib\\site-packages\\sklearn\\learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (2935849, 6) test: (214200, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import nltk, datetime\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "directory = '../data/'\n",
    "train = pd.read_csv(directory + 'sales_train_v2.csv', parse_dates=['date'], infer_datetime_format=True, dayfirst=True)\n",
    "test = pd.read_csv(directory + 'test.csv')\n",
    "submission = pd.read_csv(directory + 'sample_submission.csv')\n",
    "items = pd.read_csv(directory + 'items.csv')\n",
    "item_cats = pd.read_csv(directory + 'item_categories.csv')\n",
    "shops = pd.read_csv(directory + 'shops.csv')\n",
    "print('train:', train.shape, 'test:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we convert the raw sales data to monthly sales, broken out by item & shop\n",
    "# This placeholder dataframe will be used later to create the actual training set\n",
    "horizontal = train.groupby([train.date.apply(lambda x: x.strftime('%Y-%m')),'item_id','shop_id']).sum().reset_index()\n",
    "horizontal = horizontal[['date','item_id','shop_id','item_cnt_day']]\n",
    "horizontal = horizontal.pivot_table(index=['item_id','shop_id'], columns='date',values='item_cnt_day',fill_value=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the monthly sales data to the test data\n",
    "# This placeholder dataframe now looks similar in format to our training data\n",
    "df_test = pd.merge(test, horizontal, on=['item_id','shop_id'], how='left')\n",
    "df_test = df_test.fillna(0)\n",
    "df_test = df_test.drop(labels=['ID', 'shop_id', 'item_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we finally create the actual training set\n",
    "# Let's use the '2015-10' sales column as the target to predict\n",
    "TARGET = '2015-10'\n",
    "y_train = df_test[TARGET]\n",
    "X_train = df_test.drop(labels=[TARGET], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 1)\n",
      "(214200, 33, 1)\n"
     ]
    }
   ],
   "source": [
    "# To make the training set friendly for keras, we convert it to a numpy matrix\n",
    "X_train = X_train.as_matrix()\n",
    "X_train = X_train.reshape((214200, 33, 1))\n",
    "\n",
    "y_train = y_train.as_matrix()\n",
    "y_train = y_train.reshape(214200, 1)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 33, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lastly we create the test set by converting the test data to a numpy matrix\n",
    "# We drop the first month so that our trained LSTM can output predictions beyond the known time range\n",
    "X_test = df_test.drop(labels=['2013-01'],axis=1)\n",
    "X_test = X_test.as_matrix()\n",
    "X_test = X_test.reshape((214200, 33, 1))\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucas\\Anaconda3\\envs\\Kaggle\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 15)                1020      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 1,036\n",
      "Trainable params: 1,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model using the NestedLSTM class - two layers are a good starting point\n",
    "# Feel free to play around with the number of nodes & other model parameters\n",
    "model = Sequential()\n",
    "model.add(LSTM(15, input_shape=(33,1)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# The adam optimizer works pretty well, although you might try RMSProp as well\n",
    "model.compile(loss='mse',\n",
    "              optimizer='adam',\n",
    "              metrics=['mean_squared_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time, it is...\n",
      "Epoch 1/5\n",
      "214200/214200 [==============================] - 42s 197us/step - loss: 30.3692 - mean_squared_error: 30.3692\n",
      "Epoch 2/5\n",
      "214200/214200 [==============================] - 42s 194us/step - loss: 30.1485 - mean_squared_error: 30.1485\n",
      "Epoch 3/5\n",
      "214200/214200 [==============================] - 41s 193us/step - loss: 30.0243 - mean_squared_error: 30.0243\n",
      "Epoch 4/5\n",
      "214200/214200 [==============================] - 40s 188us/step - loss: 29.9243 - mean_squared_error: 29.9243\n",
      "Epoch 5/5\n",
      "214200/214200 [==============================] - 44s 203us/step - loss: 29.8369 - mean_squared_error: 29.8369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2758a25aef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's training time!\n",
    "BATCH_SIZE = 128\n",
    "number_of_epochs = 5\n",
    "\n",
    "print('Training time, it is...')\n",
    "model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = number_of_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test set predictions and clip values to the specified range\n",
    "y_pred = model.predict(X_test).clip(0., 20.)\n",
    "\n",
    "# Create the submission file and submit!\n",
    "preds = pd.DataFrame(y_pred, columns=['item_cnt_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Kaggle]",
   "language": "python",
   "name": "conda-env-Kaggle-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
